{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "246a9479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1be85863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Teena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Teena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Teena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Teena\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK resources\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d3e49f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.csv').fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cb325a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(text):    \n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c97d1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['comment_text'] = train_data['comment_text'].fillna(\"_na_\").apply(cleanData)\n",
    "\n",
    "#adding a column for non-toxic\n",
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train_data['none'] = 1 - train_data[label_cols].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68d88739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tag mapping\n",
    "def get_wordnet_pos(word):\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "        \"J\": wordnet.ADJ,\n",
    "        \"N\": wordnet.NOUN,\n",
    "        \"V\": wordnet.VERB,\n",
    "        \"R\": wordnet.ADV\n",
    "    }\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3daa161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex patterns\n",
    "special_character_removal = re.compile(r'[^a-z\\d ]', re.IGNORECASE)\n",
    "replace_numbers = re.compile(r'\\d+', re.IGNORECASE)\n",
    "\n",
    "def text_to_wordlist(text):\n",
    "    #Remove Special Characters\n",
    "    text=special_character_removal.sub('',text)\n",
    "    #Replace Numbers\n",
    "    text=replace_numbers.sub('_num_',text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    #remove stopwords\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "\n",
    "    # Initialize lemmatizer and using....\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmed_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in text]\n",
    "    text = \" \".join(stemmed_words)\n",
    "    \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf306f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['explanation edits make username hardcore metallica fan revert vandalism closure gas vote new york doll fac please remove template talk page since retire _num_ _num_ _num_ _num_',\n",
       " 'aww match background colour seemingly stuck thanks talk _num_ _num_ january _num_ _num_ utc',\n",
       " 'hey man really try edit war guy constantly remove relevant information talk edits instead talk page seem care format actual info',\n",
       " 'cannot make real suggestion improvement wonder section statistic later subsection type accident think reference may need tidy exact format ie date format etc later one else first preference format style reference want please let know appear backlog article review guess may delay reviewer turn list relevant form eg wikipedia good article nomination transport',\n",
       " 'sir hero chance remember page',\n",
       " 'congratulation well use tool well talk',\n",
       " 'cocksucker piss around work',\n",
       " 'vandalism matt shirvington article revert please ban',\n",
       " 'sorry word nonsense offensive anyway intend write anything article wow would jump vandalism merely request encyclopedic one use school reference selective breeding page almost stub point animal breeding short messy article give info must someone around expertise eugenics _num_ _num_ _num_ _num_',\n",
       " 'alignment subject contrary dulithgow']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = train_data['comment_text']\n",
    "x = [text_to_wordlist(i) for i in x]\n",
    "x[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51afb0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X_train = vectorizer.fit_transform(x)  # 'x' is your lemmatized training comments\n",
    "# y_train = train_data[label_cols][:100]  # Adjust slice or use full data\n",
    "\n",
    "y_train = train_data[label_cols] #target that should be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dfe425",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('data/test.csv').fillna(' ')\n",
    "test_data['content'] = test_data['content'].apply(cleanData)\n",
    "x_test = [text_to_wordlist(i) for i in test_data['content']]\n",
    "X_test = vectorizer.transform(x_test)  # Use transform, not fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5982778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = {}\n",
    "for label in label_cols:\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train, y_train[label])\n",
    "    models[label] = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e74a60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for label in label_cols:\n",
    "    preds = models[label].predict(X_test)\n",
    "\n",
    "preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d232a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_comment=\"i will kill you\"\n",
    "custom_cleaned=cleanData(custom_comment)\n",
    "processed= text_to_wordlist(custom_cleaned)\n",
    "custom_vectorized = vectorizer.transform([processed])\n",
    "custom_preds={}\n",
    "\n",
    "for label in label_cols:\n",
    "    custom_preds[label] = models[label].predict(custom_vectorized)[0]\n",
    "\n",
    "custom_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa86e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_labels = [label for label, pred in custom_preds.items() if pred == 1]\n",
    "if active_labels:   \n",
    "    label_str = ', '.join(active_labels)\n",
    "    print(f\"{custom_comment} is predicted as: {label_str}\")\n",
    "else:\n",
    "    print(f'{custom_comment} is not toxic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b00581eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"backend/toxic_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "with open(\"backend/vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d304ae0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
